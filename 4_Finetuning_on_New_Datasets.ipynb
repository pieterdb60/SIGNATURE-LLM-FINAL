{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finetuning on a New Single-Cell Dataset\n",
    "\n",
    "In this tutorial, we will demonstrate how to fine-tune an existing Cell2Sentence (C2S) model on a new single-cell RNA sequencing dataset. Fine-tuning is a crucial step in adapting a pretrained model to perform well on a specific task or dataset, improving its accuracy and generalization. This tutorial will guide you through the process of fine-tuning a C2S model to perform cell type prediction on a new dataset.\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. Load the PBMC 3K dataset from 10x (preprocessed in tutorial notebook 1)\n",
    "2. Format the dataset using a Prompt Formatter object, which prepares the data for the fine-tuning process.\n",
    "3. Load a pretrained C2S model.\n",
    "4. Fine-tune the C2S model to improve its performance on cell type prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by importing the necessary libraries. These include Python's built-in libraries, third-party libraries for handling numerical computations, progress tracking, and specific libraries for single-cell RNA sequencing data and C2S operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python built-in libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Single-cell libraries\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "\n",
    "# Cell2Sentence imports\n",
    "import cell2sentence as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Next, we will load the preprocessed dataset from the tutorial 0. This dataset has already been filtered and normalized, so it it ready for transformation into cell sentences.\n",
    "\n",
    "<font color='red'>Please make sure you have completed the preprocessing steps in Tutorial 0 before running the following code, if you are using your own dataset.</font>. Ensure that the file path is correctly set in <font color='gold'>DATA_PATH</font> to where your preprocessed data was saved from tutorial 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/pbmc3k_final.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = anndata.read_h5ad(DATA_PATH)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=\"cell_type\",\n",
    "    size=8,\n",
    "    title=\"PBMC 3K UMAP\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Cell2Sentence (CSData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_obs_cols_to_keep = [\"cell_type\",\"organism\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSData object\n",
    "arrow_ds, vocabulary = cs.CSData.adata_to_arrow(\n",
    "    adata=adata, \n",
    "    random_state=SEED, \n",
    "    sentence_delimiter=' ',\n",
    "    label_col_names=adata_obs_cols_to_keep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will consider the top 100 genes of the cell sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100  # replace with your desired number of genes\n",
    "\n",
    "arrow_ds = arrow_ds.map(lambda x: {\"cell_sentence\": \" \".join(x[\"cell_sentence\"].split()[:k])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 2000\n",
    "len(arrow_ds[sample_idx]['cell_sentence'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2s_save_dir = \"./c2s_api_testing\"  # C2S dataset will be saved into this directory\n",
    "c2s_save_name = \"PBMC_3K_tutorial2\"  # This will be the name of our C2S dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_data = cs.CSData.csdata_from_arrow(\n",
    "    arrow_dataset=arrow_ds, \n",
    "    vocabulary=vocabulary,\n",
    "    save_dir=c2s_save_dir,\n",
    "    save_name=c2s_save_name,\n",
    "    dataset_backend=\"arrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cs_data.get_sentence_strings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_data.get_sentence_strings()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import get_type_hints\n",
    "get_type_hints(cs_data.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load C2S Model\n",
    "\n",
    "Now, we will load a C2S model which will finetune on a new dataset. This model can be a LLM pretrained on natural language, or it can be a trained C2S model which will undergo further finetuning on a new dataset of interest. Typically, starting from a pretrained C2S model benefits performance, since C2S models were initialized from natural language-pretrained LLMs and trained on many single-cell datasets on different tasks.\n",
    "\n",
    "For this tutorial, we will start finetuning from the C2S-Pythia-410M cell type prediction model, which was trained to do cell type prediction on many datasets from CellxGene and Human Cell Atlas. We will finetune it for cell type prediction on our immune tissue dataset which we have loaded, which will help align the model with the cell type annotations present in this dataset as well as the expression profiles of the cells in our two donor samples. More details about the C2S-Pythia-410M cell type prediction model can be found in the Model Zoo section of the ReadME in the GitHub repo, or in the Huggingface model card.\n",
    "\n",
    "We can define our CSModel object with our pretrained cell type prediction model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CSModel object\n",
    "#cell_type_prediction_model_path = \"/home/sr2464/palmer_scratch/C2S_Files_Syed/multicell_pretraining_v2_important_models/pythia-410m-multicell_v2_2024-07-28_13-55-51_checkpoint-7600_cell_type_pred\"\n",
    "cell_type_prediction_model_path = \"/home/pieterdb/cell2sentence/c2s_models/C2S-Pythia-410m-diverse-single-and-multi-cell-tasks\"\n",
    "\n",
    "save_dir = \"./c2s_api_testing/csmodel_tutorial_3\"\n",
    "save_name = \"cell_type_pred_pythia_410M_2\"\n",
    "csmodel = cs.CSModel(\n",
    "    model_name_or_path=cell_type_prediction_model_path,\n",
    "    save_dir=save_dir,\n",
    "    save_name=save_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `model_name_or_path` parameter can be a name of a Huggingface model, for example 'EleutherAI/pythia-410m' for a 410 million parameter Pythia model pretrained on natural language (see https://huggingface.co/EleutherAI/pythia-410m), or it can be the path to a pretrained model saved on disk, as in the case in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune on new dataset\n",
    "\n",
    "Now, we will finetune our loaded C2S model on our immune tissue dataset. For training, we will need to define training arguments for finetuning our C2S model on our new dataset. Huggingface's Trainer class is used to do training, so we can utilize different training techniques (e.g. mixed precision training, gradient accumulation, gradient checkpointing, etc.) by specifying the corresponding option in the TrainingArguments object. This gives us a vast array of possible options for training, and will allow us to specify important parameters such as batch size, learning rate, and learning rate schedulers. See the full documentation for training arguments at:\n",
    "- https://huggingface.co/docs/transformers/en/main_classes/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our training task, which in our case will be cell type prediction. Possible values for the training task parameter can be found in the `prompt_formatter.py` file in the source code, under `SUPPORTED_TASKS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_task = \"cell_type_prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a datetimestamp to mark our training session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimestamp = datetime.now().strftime('%Y-%m-%d-%H_%M_%S')\n",
    "output_dir = os.path.join(csmodel.save_dir, datetimestamp + f\"_finetune_{training_task}\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, we define our training arguments. For this tutorial, we will use a batch size of 8 with 4 gradient accumulation steps, yielding an effective batch size of 32. We will use a learning rate of 1e-5 with a cosine annealing scheduler, and we will train for 5 epochs total. Some other important parameters specified here are:\n",
    "- bf16: Uses mixed-precision training with bfloat16 dtype\n",
    "- logging_steps: controls how often we log training loss\n",
    "- eval_steps: controls how often we run the eval loop\n",
    "- warmup_ratio: percentage of training in which learning rate warms up to the base learning rate specified\n",
    "\n",
    "Full explanations of all possible training arguments can be found in the Huggingface Trainer documentation: \n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=5, \n",
    "    eval_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    warmup_ratio=0.05,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csmodel.fine_tune(\n",
    "    csdata=cs_data,\n",
    "    task=training_task,\n",
    "    train_args=train_args,\n",
    "    loss_on_response_only=False,\n",
    "    top_k_genes=100,\n",
    "    max_eval_samples=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trained models are now saved in the output directory we specified in the training arguments. Huggingface will save the latest checkpoints of the training session, and will also keep the checkpoint which has the lowest validation loss.\n",
    "\n",
    "\n",
    "[Go to Notebook 5 →](./5_Cell_Type_Prediction.ipynb) to see how to run cell type prediction inference with our trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell2sentence_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
